{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91440d61",
   "metadata": {},
   "source": [
    "## Scraping non-tabular, multipage sites\n",
    "Scrape the top 500 <a href=\"https://bestsellingalbums.org/decade/2010\">best-selling albums of the 2010's</a>. Your data must include the following datapoints:\n",
    "\n",
    "- Name of album\n",
    "- Name of artist\n",
    "- Number of albums sold \n",
    "- The link to the page that breaks down sales by country (found by clicking album title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9a01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fb89c",
   "metadata": {},
   "source": [
    "## Old solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ed66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [] ## hold all dfs\n",
    "url = \"https://bestsellingalbums.org/decade/2010\" ## base url\n",
    "\n",
    "count = 1 ## count\n",
    "while count <=10:\n",
    "    print(f\"Scraping {url}\")\n",
    "    ## get response\n",
    "    response = requests.get(url)\n",
    "    ## turn response into soup (navigable html from string)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    ## grab ALL albums data and store in variable\n",
    "    all_albums = soup.find_all(\"div\", class_=\"album_card\")\n",
    "\n",
    "    ## name lists to hold data\n",
    "    artists_list = []\n",
    "    albums_list = []\n",
    "    albums_url_list = []\n",
    "    sales_list = []\n",
    "\n",
    "    ## iterate through to capture target data points\n",
    "    for target in all_albums:\n",
    "        ## artist name\n",
    "        artists_list.append(target.find(\"div\", class_=\"artist\").get_text())\n",
    "        ## album title\n",
    "        albums_list.append(target.find(\"div\", class_=\"album\").get_text())\n",
    "        ## album links\n",
    "        albums_url_list.append(target.find(\"a\").get(\"href\"))\n",
    "        ##sales\n",
    "        sales = target.find(\"div\", class_=\"sales\").get_text() ## get the sales text\n",
    "        sales = int(sales.replace(\"Sales: \",\"\").replace(\",\",\"\")) ## Turn into integer remove Sales: and commas\n",
    "        sales_list.append(sales)\n",
    "\n",
    "    ## zip to tuple\n",
    "    album_data = []\n",
    "    for all_data in zip(artists_list, albums_list, sales_list, albums_url_list):\n",
    "        album_data.append(all_data)\n",
    "\n",
    "        #   convert to df\n",
    "    df = pd.DataFrame(album_data)\n",
    "    df.columns = [\"artist\", \"title\", \"sales\", \"more_info\"]\n",
    "    all_dfs.append(df)\n",
    "\n",
    "    ## incredment url and set timer\n",
    "    count += 1\n",
    "    url = \"https://bestsellingalbums.org/decade/2010\"\n",
    "    url = f\"{url}-{count}\"\n",
    "    snoozer = randrange(5,12)\n",
    "    print(f\"snoozing for {snoozer} seconds before next scrape\")\n",
    "    time.sleep(snoozer)\n",
    "    \n",
    "print(\"done scraping all links\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e1725",
   "metadata": {},
   "source": [
    "## REFACTORED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3034c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DF from page 1 and snoozing for 5 seconds before next page\n",
      "Created DF from page 2 and snoozing for 9 seconds before next page\n",
      "Created DF from page 3 and snoozing for 5 seconds before next page\n",
      "Created DF from page 4 and snoozing for 11 seconds before next page\n",
      "Created DF from page 5 and snoozing for 9 seconds before next page\n",
      "Created DF from page 6 and snoozing for 5 seconds before next page\n",
      "Created DF from page 7 and snoozing for 11 seconds before next page\n",
      "Created DF from page 8 and snoozing for 6 seconds before next page\n",
      "Created DF from page 9 and snoozing for 10 seconds before next page\n",
      "Created DF from page 10 and snoozing for 10 seconds before next page\n",
      "Done scraping all 10 pages\n"
     ]
    }
   ],
   "source": [
    "all_dfs = [] ## hold all dfsb\n",
    "base_url = \"https://bestsellingalbums.org/decade/2010\" ## base url\n",
    "end_page = 10 ## how many pages we want to scrape\n",
    "\n",
    "for url_number in range(1, end_page + 1):\n",
    "    try: ## anything but the first page\n",
    "        if url_number != 1:\n",
    "            response = requests.get(f\"{base_url}-{url_number}\")\n",
    "        else:\n",
    "            response = requests.get(base_url)\n",
    "    except: ## problematic first page\n",
    "        print(f\"Problem with {base_url}-{url_number}\")\n",
    "    finally: ## turn response into soup (navigable html from string)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        ## grab ALL target data and store in lists\n",
    "        all_targets = soup.find_all(\"div\", class_=\"album_card\")\n",
    "        artists_list = [target.find(\"div\", class_=\"artist\").get_text() for target in all_targets]\n",
    "        albums_list = [target.find(\"div\", class_=\"album\").get_text() for target in all_targets]\n",
    "        more_info_list = [target.find(\"a\").get(\"href\") for target in all_targets]\n",
    "        sales_list = [int(target.find(\"div\", class_=\"sales\").get_text().replace(\"Sales: \", \"\").replace(\",\", \"\"))\\\n",
    "                      for target in all_targets]\n",
    "        \n",
    "        ## create a dictionary using captured lists\n",
    "        all_dfs.append(pd.DataFrame({\"artist\": artists_list, \"album\": albums_list,\n",
    "                           \"sales\": sales_list, \"more_info\": more_info_list}))\n",
    "            \n",
    "        ## timer\n",
    "        snoozer = randrange(5,12)\n",
    "        print(f\"Created DF from page {url_number} and snoozing for {snoozer} seconds before next page\")\n",
    "        time.sleep(snoozer)## set timer\n",
    "print(f\"Done scraping all {end_page} pages\")                                \n",
    "                          \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d031620d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>sales</th>\n",
       "      <th>more_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADELE</td>\n",
       "      <td>21</td>\n",
       "      <td>30000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADELE</td>\n",
       "      <td>25</td>\n",
       "      <td>23000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MICHAEL BUBLÉ</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>15000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/30524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAYLOR SWIFT</td>\n",
       "      <td>1989</td>\n",
       "      <td>14748116</td>\n",
       "      <td>https://bestsellingalbums.org/album/45488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUSTIN BIEBER</td>\n",
       "      <td>PURPOSE</td>\n",
       "      <td>14000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/23318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>JUSTIN BIEBER</td>\n",
       "      <td>UNDER THE MISTLETOE</td>\n",
       "      <td>2700725</td>\n",
       "      <td>https://bestsellingalbums.org/album/23319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>AC/DC</td>\n",
       "      <td>ROCK OR BUST</td>\n",
       "      <td>2700000</td>\n",
       "      <td>https://bestsellingalbums.org/album/901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>PITBULL</td>\n",
       "      <td>PLANET PIT</td>\n",
       "      <td>2683000</td>\n",
       "      <td>https://bestsellingalbums.org/album/36403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>STROMAE</td>\n",
       "      <td>RACINE CARREE</td>\n",
       "      <td>2657500</td>\n",
       "      <td>https://bestsellingalbums.org/album/44563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>GOTYE</td>\n",
       "      <td>MAKING MIRRORS</td>\n",
       "      <td>2655000</td>\n",
       "      <td>https://bestsellingalbums.org/album/17329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist                album     sales  \\\n",
       "0            ADELE                   21  30000000   \n",
       "1            ADELE                   25  23000000   \n",
       "2    MICHAEL BUBLÉ            CHRISTMAS  15000000   \n",
       "3     TAYLOR SWIFT                 1989  14748116   \n",
       "4    JUSTIN BIEBER              PURPOSE  14000000   \n",
       "..             ...                  ...       ...   \n",
       "145  JUSTIN BIEBER  UNDER THE MISTLETOE   2700725   \n",
       "146          AC/DC         ROCK OR BUST   2700000   \n",
       "147        PITBULL           PLANET PIT   2683000   \n",
       "148        STROMAE        RACINE CARREE   2657500   \n",
       "149          GOTYE       MAKING MIRRORS   2655000   \n",
       "\n",
       "                                     more_info  \n",
       "0     https://bestsellingalbums.org/album/1034  \n",
       "1     https://bestsellingalbums.org/album/1035  \n",
       "2    https://bestsellingalbums.org/album/30524  \n",
       "3    https://bestsellingalbums.org/album/45488  \n",
       "4    https://bestsellingalbums.org/album/23318  \n",
       "..                                         ...  \n",
       "145  https://bestsellingalbums.org/album/23319  \n",
       "146    https://bestsellingalbums.org/album/901  \n",
       "147  https://bestsellingalbums.org/album/36403  \n",
       "148  https://bestsellingalbums.org/album/44563  \n",
       "149  https://bestsellingalbums.org/album/17329  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## turn into data frame\n",
    "df = pd.concat(all_dfs, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49759f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
