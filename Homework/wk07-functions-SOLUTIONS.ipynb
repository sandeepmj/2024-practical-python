{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5320cbbf",
   "metadata": {},
   "source": [
    "# Homework for Week 7\n",
    "## Functions and Downloading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5db5b3",
   "metadata": {},
   "source": [
    "### Write modular functions for the following:\n",
    "\n",
    "1. Making a ```request```\n",
    "2. Converting a ```response``` into ```soup```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b96b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. code here for requests function (create cells if/as needed)\n",
    "import requests\n",
    "\n",
    "def make_requests(url):\n",
    "    '''\n",
    "    This function makes a request to a website\n",
    "    para1: a url to scrape\n",
    "    returns a response if status is 200 or more but less than 400\n",
    "    return status code if busted\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    if 200 <=response.status_code < 400:\n",
    "        return response\n",
    "    else:\n",
    "        return response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed087ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. code here for response function (create cells if/as needed)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(response):\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350b62c",
   "metadata": {},
   "source": [
    "### 3. Demo downloading files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files <a href=\"https://sandeepmj.github.io/scrape-example-page/pages.html\">on this site</a>. During class we downloaded on e set of text files and one set of PDF files.\n",
    "\n",
    "Now download **ALL files at one time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a209478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here (create cells if/as needed)\n",
    "\n",
    "## we made import above, but still need a few\n",
    "import time\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e10706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## url to scrape\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff80760",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use our functions to download response and turn to soup\n",
    "response = make_requests(url)\n",
    "soup = make_soup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c53165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this first set of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       " </ul>,\n",
       " <ul class=\"pdfs downloadable\">\n",
       " <p class=\"pages\">Download this list of PDFs</p>\n",
       " <li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       " <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       " </ul>,\n",
       " <ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this second set of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_A.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_B.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_C.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_D.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_E.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_F.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_G.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_H.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_I.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_J.txt\">10</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## target ul downloadable to hold all target data\n",
    "all_target = soup.find_all(\"ul\", class_=\"downloadable\")\n",
    "all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e46f6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_02.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_03.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_04.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_05.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_06.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_07.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_08.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_09.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_10.txt\">10</a>],\n",
       " [<a href=\"files/pdf_1.pdf\">1</a>,\n",
       "  <a href=\"files/pdf_2.pdf\">2</a>,\n",
       "  <a href=\"files/pdf_3.pdf\">3</a>,\n",
       "  <a href=\"files/pdf_4.pdf\">4</a>,\n",
       "  <a href=\"files/pdf_5.pdf\">5</a>,\n",
       "  <a href=\"files/pdf_6.pdf\">6</a>,\n",
       "  <a href=\"files/pdf_7.pdf\">7</a>,\n",
       "  <a href=\"files/pdf_8.pdf\">8</a>,\n",
       "  <a href=\"files/pdf_9.pdf\">9</a>,\n",
       "  <a href=\"files/pdf_10.pdf\">10</a>],\n",
       " [<a href=\"files/text_doc_A.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_B.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_C.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_D.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_E.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_F.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_G.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_H.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_I.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_J.txt\">10</a>]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## gather all atags\n",
    "aTags = [atag.find_all(\"a\") for atag in all_target]\n",
    "aTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e289cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b3e7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>,\n",
       " <a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>,\n",
       " <a href=\"files/text_doc_A.txt\">1</a>,\n",
       " <a href=\"files/text_doc_B.txt\">2</a>,\n",
       " <a href=\"files/text_doc_C.txt\">3</a>,\n",
       " <a href=\"files/text_doc_D.txt\">4</a>,\n",
       " <a href=\"files/text_doc_E.txt\">5</a>,\n",
       " <a href=\"files/text_doc_F.txt\">6</a>,\n",
       " <a href=\"files/text_doc_G.txt\">7</a>,\n",
       " <a href=\"files/text_doc_H.txt\">8</a>,\n",
       " <a href=\"files/text_doc_I.txt\">9</a>,\n",
       " <a href=\"files/text_doc_J.txt\">10</a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## flatten lists with outer list\n",
    "all_aTags = list(itertools.chain(*aTags))\n",
    "all_aTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34359b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_A.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_B.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_C.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_D.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_E.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_F.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_G.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_H.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_I.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_J.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## base url\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\"\n",
    "\n",
    "## extra hrefs from each atag\n",
    "links = [base_url + atag.get(\"href\") for atag in all_aTags]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bd91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import wget\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5143935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make timer function\n",
    "def snoozer(start_range, end_range):\n",
    "    snooze_time = randrange(start_range, end_range)\n",
    "    print(f\"\\n Snoozing for {snooze_time} seconds\")\n",
    "    return time.sleep(snooze_time)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34aa858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 30\n",
      "100% [................................................................] 76 / 76\n",
      " Snoozing for 19 seconds\n",
      "Downloading link 2 of 30\n",
      "100% [................................................................] 66 / 66\n",
      " Snoozing for 11 seconds\n",
      "Downloading link 3 of 30\n",
      "100% [................................................................] 70 / 70\n",
      " Snoozing for 15 seconds\n",
      "Downloading link 4 of 30\n",
      "100% [................................................................] 63 / 63\n",
      " Snoozing for 11 seconds\n",
      "Downloading link 5 of 30\n",
      "100% [................................................................] 66 / 66\n",
      " Snoozing for 10 seconds\n",
      "Downloading link 6 of 30\n",
      "100% [................................................................] 66 / 66\n",
      " Snoozing for 18 seconds\n",
      "Downloading link 7 of 30\n",
      "100% [................................................................] 69 / 69\n",
      " Snoozing for 15 seconds\n",
      "Downloading link 8 of 30\n",
      "100% [................................................................] 65 / 65\n",
      " Snoozing for 12 seconds\n",
      "Downloading link 9 of 30\n",
      "100% [................................................................] 66 / 66\n",
      " Snoozing for 17 seconds\n",
      "Downloading link 10 of 30\n",
      "100% [................................................................] 60 / 60\n",
      " Snoozing for 19 seconds\n",
      "Downloading link 11 of 30\n",
      "100% [..........................................................] 12812 / 12812\n",
      " Snoozing for 17 seconds\n",
      "Downloading link 12 of 30\n",
      "100% [..........................................................] 12897 / 12897\n",
      " Snoozing for 12 seconds\n",
      "Downloading link 13 of 30\n",
      "100% [..........................................................] 12908 / 12908\n",
      " Snoozing for 11 seconds\n",
      "Downloading link 14 of 30\n",
      "100% [..........................................................] 12843 / 12843\n",
      " Snoozing for 14 seconds\n",
      "Downloading link 15 of 30\n",
      "100% [..........................................................] 12881 / 12881\n",
      " Snoozing for 14 seconds\n",
      "Downloading link 16 of 30\n",
      "100% [..........................................................] 12906 / 12906\n",
      " Snoozing for 12 seconds\n",
      "Downloading link 17 of 30\n",
      "100% [..........................................................] 12816 / 12816\n",
      " Snoozing for 11 seconds\n",
      "Downloading link 18 of 30\n",
      "100% [..........................................................] 12921 / 12921\n",
      " Snoozing for 11 seconds\n",
      "Downloading link 19 of 30\n",
      "100% [..........................................................] 12901 / 12901\n",
      " Snoozing for 12 seconds\n",
      "Downloading link 20 of 30\n",
      "100% [..........................................................] 13049 / 13049\n",
      " Snoozing for 14 seconds\n",
      "Downloading link 21 of 30\n",
      "100% [................................................................] 69 / 69\n",
      " Snoozing for 18 seconds\n",
      "Downloading link 22 of 30\n",
      "100% [................................................................] 61 / 61\n",
      " Snoozing for 20 seconds\n",
      "Downloading link 23 of 30\n",
      "100% [................................................................] 72 / 72\n",
      " Snoozing for 17 seconds\n",
      "Downloading link 24 of 30\n",
      "100% [................................................................] 68 / 68\n",
      " Snoozing for 15 seconds\n",
      "Downloading link 25 of 30\n",
      "100% [................................................................] 70 / 70\n",
      " Snoozing for 16 seconds\n",
      "Downloading link 26 of 30\n",
      "100% [................................................................] 66 / 66\n",
      " Snoozing for 15 seconds\n",
      "Downloading link 27 of 30\n",
      "100% [................................................................] 76 / 76\n",
      " Snoozing for 16 seconds\n",
      "Downloading link 28 of 30\n",
      "100% [................................................................] 71 / 71\n",
      " Snoozing for 19 seconds\n",
      "Downloading link 29 of 30\n",
      "100% [................................................................] 63 / 63\n",
      " Snoozing for 10 seconds\n",
      "Downloading link 30 of 30\n",
      "100% [................................................................] 71 / 71\n",
      " Snoozing for 19 seconds\n"
     ]
    }
   ],
   "source": [
    "## downlaod docs from each document.\n",
    "link_count = 1\n",
    "start_range, end_range = 10, 21\n",
    "for link in links:\n",
    "    print(f\"Downloading link {link_count} of {len(links)}\")\n",
    "    link_count += 1\n",
    "    wget.download(link)\n",
    "    snoozer(start_range, end_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeed1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
